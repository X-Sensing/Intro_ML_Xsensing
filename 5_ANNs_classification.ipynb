{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Artifical neural networks (ANNs) are applied as a supervised learning technique to classify data into known classes.\n",
    "\n",
    "![SegmentLocal](Images/curve_fit.png \"curvefit\")\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "y & = W_1\\,x4 + W_2\\,x^3 + W_3\\,x^2 + W_4\\,x + W_5\\\\\n",
    "\\end{align}\n",
    "\n",
    "You can think of them as a cousin of familiar regression or curve-fitting, but taken to higher dimensions. For example, when fitting a polynomial curve to a dataset we learn the values of the coefficients $W$ so that the model curve can accurately predict an outcome given input data. Similarly, for an ANN classifier, we learn a set of weights (coefficients) between the neurons so the the network can accurately predict a label for input data.\n",
    "\n",
    "![SegmentLocal](Images/ANN_1layer.svg \"ANN1\")\n",
    "\n",
    "In both cases the fitting procedure employs a measure of how well model performs. For curve fitting the distance between the data and model is minimised (e.g., $\\chi^2$ or least-squared distance), while for an ANN classifier a *loss function* is minimised.\n",
    "\n",
    "When training the classifier, the loss function desires the following state:\n",
    "\n",
    "<img src=\"Images/loss_margin.jpg\">\n",
    "\n",
    "Scores inside the red region (or higher) will accumulate a loss, while scores below will accumulate zero loss.\n",
    "\n",
    "## Stochastic Gradient Descent and mini-batches\n",
    "\n",
    "Minimisation is done iteratively vis gradient descent:\n",
    " * Randomly initialise the weights between neurons\n",
    " * assess the performance of the network using the loss function\n",
    " * examine the slope of the loss surface and jump 'down-hill'\n",
    " * rinse and repeat until no improvement is seen\n",
    "\n",
    "![SegmentLocal](Images/grad_descent.gif \"descent\")\n",
    "\n",
    "This is superficially similar fitting a polynomial curve, however, there are two major differences:\n",
    " 1. the weight coeefficients are not independent - the output of neurons are affected by all in the chain before them\n",
    " 1. the very large datasets required by ANNs will not fit in memory\n",
    " \n",
    "The first probem is addressed by back-probagating the gradients using the chain-rule. The second problem is solved by only training on a subset of the data, called a 'mini-batch' (or just 'batch'), and randomly changing the selection of this data every cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ANNs using Keras\n",
    "\n",
    "In the last 10 years there has been an explosion in the availability of software for [deep learning](https://en.wikipedia.org/wiki/Comparison_of_deep-learning_software). One of the most popular is [Keras](https://keras.io/), which is a layer on top of Google's [TensorFlow](https://www.tensorflow.org/versions) machine learning and matrix manipulation library. (Alternatives are PyTorch, developed by Facebook, and CNTK by Microsoft). \n",
    "\n",
    "Performing the gradient descent step on even moderately large ANNs requires very intensive calculations. TensorFlow (and Keras) can take advantage of the parallel processing power offered by Graphics Processing Units (GPUs) to vastly accelerate the training step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying handwriting - the MNIST dataset\n",
    "\n",
    "Historically, one of the simplest tasks ANNs were applied to was handwriting recognition. In this notebook we will build an ANN to classify small images of handwritten digits [0 - 9]. This is a classic entry-level problem in machine learning.\n",
    "\n",
    "The Modified National Institute of Standards and Technology (MNIST) dataset contains 60,000 training images and 10,000 testing images of handwritten digits as 28x28 pixel black & white images.\n",
    "\n",
    "![SegmentLocal](Images/MNIST_grid.png \"MNIST_grid\")\n",
    "\n",
    "These images are encoded with intensity values [0 - 255], so we usually scale them [0 - 1] by dividing by 255.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the classifier\n",
    "\n",
    "Here we build a simple neural network classifier with two 'hidden layers'. The number of neurons in the *input layer* corresponds to each pixel of an image fed into the network. The number of neurons in the *output layer* corresponds to the possible classes.\n",
    "\n",
    "![SegmentLocal](Images/ANN_2layers.png \"ANN2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience the MNIST data is stored on disk as a Numpy save file containing a 3D array (images) or 1D array (labels 0 - 9). It is small enough to fit into memory. In this case, the data is already split into training ans tets sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the training images and labels from Numpy save files\n",
    "trainX = np.load('DATA/minimnist/train_images.npy')\n",
    "trainY = np.load('DATA/minimnist/train_labels.npy')\n",
    "print(\"Training array shape:\", trainX.shape)\n",
    "print(\"Labels shape\", trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing images and labels from Numpy save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an individual image and display some statistics of the inetensity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Show a randomly chosen image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and test data should be converted into floating point precision and scaled into the range [0 - 1]. Machine learning algorithms tend to expect data in this format, unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the intensities [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with Scikit Learn, we need to re-encode our labels as one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Create one-hot label vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to flatten each image into a vector of 784 pixels so that it can be injected into the bottom layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train and test image stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define the architecture of our network. Keras supports the creation of complicated networks with multiple input or output branchs. However, we will define a simple straight-through network with two hidden layers containing 256 and 128 neurons, respectively. There should be 784 input neurons corresponding to the number of pixels in an image (28 * 28) and 10 output neurons, one for each digit.\n",
    "\n",
    "In Keras, the 'Dense' layer represents a layer of neurons where each neuron will be connected to all other neurons in layers immediately above and below. \n",
    "\n",
    "The activation function scales the output of the neuron in a non-linear way, allowing the network to model non-linear systems.\n",
    "\n",
    "![SegmentLocal](Images/SigmoidFunction.png \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "# Define 784-256-128-10 architecture using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been defined we need to pick a loss function to assess it's performance and run the gradient descent algorithm. There are also a number of tweaked optimiser algorithms avaialable, however SGD works fine as a default. \n",
    "\n",
    "The optimiser also accepts a 'learning rate', a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "At this point we wlso need to choose a batch size (how much data can we fit in memory?) and number of ephocs to train for (how many times do we push a batch throughthe network to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Typical learning rate for SGD\n",
    "learnRate = 0.01\n",
    "opt = SGD(learnRate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# The run the stochastic gradient descent algorithm\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "              epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Training has finished and our model is ready to make prediction about the testing data. Let's just make a prediction about one image to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single image - a batch of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn comes with a handy function to report useful classification statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(testX, batch_size=128)\n",
    "\n",
    "# Print a formatted report\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=[str(x) for x in lb.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve an average precision of 89 %, which is not bad compared to our random chance of 1/10.\n",
    "\n",
    "Let's go back and plot some key curves for the training process - loss and accuracy. These were stored in our history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
